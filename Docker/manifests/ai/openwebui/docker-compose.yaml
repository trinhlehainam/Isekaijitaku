# NOTE: Ollama slow loading models problems running on wsl2 docker
# https://www.reddit.com/r/ollama/comments/1awq12e/memory_allocation_is_slow_loading_models_in/

services:
  # https://github.com/open-webui/open-webui?tab=readme-ov-file#installation-with-default-configuration
  openwebui:
    container_name: openwebui
    # https://github.com/open-webui/open-webui/pkgs/container/open-webui
    image: ghcr.io/open-webui/open-webui:cuda
    restart: unless-stopped
    ports:
      - "3000:8080"
    # extra_hosts:
    #   - "host.docker.internal:host-gateway"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    # environment:
    #   - 'OLLAMA_BASE_URL=https://example.com:11434'
    #   - 'ENABLE_LITELLM=false'
    volumes:
      - open-webui-local:/app/backend/data
    networks:
      - proxy
    depends_on:
      - ollama

  # https://registry.hub.docker.com/r/ollama/ollama
  ollama:
    container_name: ollama
    # https://hub.docker.com/r/ollama/ollama/tags
    image: ollama/ollama:0.5.1
    restart: unless-stopped
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    volumes:
      - ollama-local:/root/.ollama
    networks:
      - proxy

volumes:
  open-webui-local:
    driver: local
  ollama-local:
    driver: local

networks:
  proxy:
    external: true
