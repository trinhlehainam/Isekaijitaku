# NOTE: Ollama slow loading models problems running on wsl2 docker
# https://www.reddit.com/r/ollama/comments/1awq12e/memory_allocation_is_slow_loading_models_in/

services:
  # https://github.com/open-webui/open-webui?tab=readme-ov-file#installation-with-default-configuration
  openwebui:
    container_name: openwebui
    # https://github.com/open-webui/open-webui/pkgs/container/open-webui
    image: ghcr.io/open-webui/open-webui:v0.6.2-cuda
    restart: unless-stopped
    depends_on:
      - ollama
    # ports:
    #   - "8080:8080"
    # extra_hosts:
    #   - "host.docker.internal:host-gateway"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    environment:
      OLLAMA_API_URL: http://ollama:11434
      # # https://docs.openwebui.com/getting-started/env-configuration#ollama
      # ENABLE_OLLAMA_API: false
      # # https://docs.openwebui.com/tutorials/web-search/searxng/#2-alternative-setup
      # ENABLE_RAG_WEB_SEARCH: True
      # RAG_WEB_SEARCH_ENGINE: "searxng"
      # RAG_WEB_SEARCH_RESULT_COUNT: 3
      # RAG_WEB_SEARCH_CONCURRENT_REQUESTS: 10
      # SEARXNG_QUERY_URL: "http://searxng:8080/search?q=<query>"
    volumes:
      - open-webui-local:/app/backend/data
    networks:
      - proxy
      - ollama
      # - searxng
    labels:
      - "traefik.enable=true"
      - 'traefik.docker.network=proxy'
      - 'traefik.http.services.openwebui.loadbalancer.server.port=8080'
      - "traefik.http.routers.openwebui-local.rule=Host(`openwebui.yourdomain.local`)"
      - "traefik.http.routers.openwebui-local.entrypoints=websecure"
      - "traefik.http.routers.openwebui-local.tls=true"
      - "traefik.http.routers.openwebui-local.tls.certresolver=stepca"

  # https://registry.hub.docker.com/r/ollama/ollama
  ollama:
    container_name: ollama
    # https://hub.docker.com/r/ollama/ollama/tags
    image: ollama/ollama:0.6.2
    restart: unless-stopped
    # ports:
    #   - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    volumes:
      - ollama-local:/root/.ollama
    networks:
      - proxy
      - ollama

volumes:
  open-webui-local:
    driver: local
  ollama-local:
    driver: local

networks:
  proxy:
    external: true
  ollama:
    driver: bridge
  # searxng:
  #   external: true
